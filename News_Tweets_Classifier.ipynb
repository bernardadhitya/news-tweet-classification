{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Tweets_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjYjVIM8aI67"
      },
      "source": [
        "# News Tweets Classifier\n",
        "Reference:\n",
        "- https://iq.opengenus.org/text-classification-using-k-nearest-neighbors/\n",
        "- https://www.geeksforgeeks.org/saving-a-machine-learning-model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9JGq7zHaOQn"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52en3cjUZ1kc",
        "outputId": "69d2e68c-8d70-4e16-89a7-95ee7383c8f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLb8uen0aJIK"
      },
      "source": [
        "Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ6IPV-dabWO",
        "outputId": "e59ae44c-1033-47ba-af0e-6a6c22c047fb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RjNWJ20chfa"
      },
      "source": [
        "Import and preprocess dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n26FXYXckmC"
      },
      "source": [
        "FILENAME = \"/content/drive/My Drive/News Tweets Classifier/news_tweets_labeled.csv\"\n",
        "df = pd.read_csv(FILENAME)\n",
        "df = df[[\"text\", \"category\"]]\n",
        "# df = df.sort_values([\"category\", \"text\"], ascending=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AkjZg8jxL6H"
      },
      "source": [
        "sw = stopwords.words('english')\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "for i in range(df.shape[0]):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', df.loc[i, 'text'])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "\n",
        "  review = [wnl.lemmatize(word) for word in review if not word in sw]\n",
        "  review = ' '.join(review)\n",
        "  df.loc[i, 'text'] = review"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX_yvDbGc8SE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe68eee-e17d-4812-c828-b4ae9261686b"
      },
      "source": [
        "df = df[:3000]\n",
        "print(df.groupby('category').count())\n",
        "train_data = df.sample(frac=0.8, random_state=200)\n",
        "test_data = df.drop(train_data.index)\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               text\n",
            "category           \n",
            "business        290\n",
            "entertainment   250\n",
            "health          283\n",
            "lifestyle       326\n",
            "others          808\n",
            "politics        596\n",
            "science         126\n",
            "sport           216\n",
            "technology       52\n",
            "weather          53\n",
            "2400\n",
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHNdHAtweQ4b"
      },
      "source": [
        "Build dictionary and transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3WkZ_kaeVYF"
      },
      "source": [
        "# Builds a dictionary of features and transforms document to feature vectors and convert tweets to a \n",
        "# matrix of token counts (CountVectorizer)\n",
        "count_vect = CountVectorizer()\n",
        "x_train_counts = count_vect.fit_transform(train_data['text'])\n",
        "\n",
        "# Transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cScZpVUMggUN"
      },
      "source": [
        "Train the model and give some new tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-2JdqdPgDjG"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
        "\n",
        "# Train classifier; train_data['category'] will be having numbers assigned for each category in train data\n",
        "clf = knn.fit(x_train_tfidf, train_data['category'])\n",
        "\n",
        "# Input data to predict their classes of the given categories\n",
        "tweets_new = [\"Seven people have been arrested following a search of two vehicles in Hayle Cornwall police have said\", \n",
        "            \"RT MoneyTelegraph Your gas and electricity bills are set to increase by 36pc over the next 10 years\",\n",
        "            \"The most common symptom of Covid19 is now a headache say experts as they warned people to get tested even if they think they are not suffering from the illness\"]\n",
        "# building up feature vector of input\n",
        "x_new_counts = count_vect.transform(tweets_new)\n",
        "# Call transform instead of fit_transform because it's already been fit\n",
        "x_new_tfidf = tfidf_transformer.transform(x_new_counts)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXqP7i0YiWo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764c6a80-d37a-477e-af14-f341d754219d"
      },
      "source": [
        "# Predicting the category of input text: Will give out number of category\n",
        "predicted = clf.predict(x_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "  print('%r => %s' %(doc, category))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Seven people have been arrested following a search of two vehicles in Hayle Cornwall police have said' => others\n",
            "'RT MoneyTelegraph Your gas and electricity bills are set to increase by 36pc over the next 10 years' => business\n",
            "'The most common symptom of Covid19 is now a headache say experts as they warned people to get tested even if they think they are not suffering from the illness' => health\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpSQ4x2Bp47O"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne_xXqtgp6EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3f28ca-e950-4c48-e1bf-84177c3a5cf7"
      },
      "source": [
        "# use Pipeline to add vectorizer -> transformer -> classifier all in a one compound classifier\n",
        "tweet_clf = Pipeline([\n",
        "  ('vect', CountVectorizer()),\n",
        "  ('tfidf', TfidfTransformer()),\n",
        "  ('clf', knn)\n",
        "])\n",
        "\n",
        "# Fitting train data to the pipeline\n",
        "tweet_clf.fit(train_data['text'], train_data['category'])\n",
        "\n",
        "# Test data\n",
        "docs_test = test_data['text']\n",
        "\n",
        "# Predicting test data\n",
        "predicted = tweet_clf.predict(docs_test)\n",
        "print('We got an accuracy of', np.mean(predicted == test_data['category']) * 100, '% over the test data.')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We got an accuracy of 75.5 % over the test data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7fC0bFUBxXa"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FHx0CYByqR"
      },
      "source": [
        "classifier_model = pickle.dumps(tweet_clf)"
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}