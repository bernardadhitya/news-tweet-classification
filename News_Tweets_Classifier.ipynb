{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Tweets_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjYjVIM8aI67"
      },
      "source": [
        "# News Tweets Classifier\n",
        "Reference:\n",
        "- https://iq.opengenus.org/text-classification-using-k-nearest-neighbors/\n",
        "- https://www.geeksforgeeks.org/saving-a-machine-learning-model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9JGq7zHaOQn"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52en3cjUZ1kc",
        "outputId": "0c14b18c-4988-461a-d797-9ac909df8d67"
      },
      "source": [
        "!wget \"https://github.com/bernardadhitya/news-tweet-classification/raw/master/news_tweets_labeled.csv\"\n",
        "!wget \"https://github.com/bernardadhitya/news-tweet-classification/raw/master/preprocessor.py\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 11:03:36--  https://github.com/bernardadhitya/news-tweet-classification/raw/master/news_tweets_labeled.csv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/bernardadhitya/news-tweet-classification/master/news_tweets_labeled.csv [following]\n",
            "--2021-07-17 11:03:36--  https://raw.githubusercontent.com/bernardadhitya/news-tweet-classification/master/news_tweets_labeled.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1637258 (1.6M) [text/plain]\n",
            "Saving to: ‘news_tweets_labeled.csv’\n",
            "\n",
            "news_tweets_labeled 100%[===================>]   1.56M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-07-17 11:03:36 (25.1 MB/s) - ‘news_tweets_labeled.csv’ saved [1637258/1637258]\n",
            "\n",
            "--2021-07-17 11:03:36--  https://github.com/bernardadhitya/news-tweet-classification/raw/master/preprocessor.py\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/bernardadhitya/news-tweet-classification/master/preprocessor.py [following]\n",
            "--2021-07-17 11:03:37--  https://raw.githubusercontent.com/bernardadhitya/news-tweet-classification/master/preprocessor.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 551 [text/plain]\n",
            "Saving to: ‘preprocessor.py’\n",
            "\n",
            "preprocessor.py     100%[===================>]     551  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-17 11:03:37 (28.0 MB/s) - ‘preprocessor.py’ saved [551/551]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLb8uen0aJIK"
      },
      "source": [
        "Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ6IPV-dabWO",
        "outputId": "dc33b258-d10c-457e-a97b-74b01117ef17"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from preprocessor import Preprocessor"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvEO-Pm4HTGt"
      },
      "source": [
        "preprocessor = Preprocessor()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RjNWJ20chfa"
      },
      "source": [
        "Import and preprocess dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n26FXYXckmC"
      },
      "source": [
        "df = pd.read_csv('news_tweets_labeled.csv')\n",
        "df = df[[\"text\", \"category\"]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AkjZg8jxL6H"
      },
      "source": [
        "sw = stopwords.words('english')\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "for i in range(df.shape[0]):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', df.loc[i, 'text'])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "\n",
        "  review = [wnl.lemmatize(word) for word in review if not word in sw]\n",
        "  review = ' '.join(review)\n",
        "  df.loc[i, 'text'] = review"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX_yvDbGc8SE"
      },
      "source": [
        "df = preprocessor.prepare_dataset(df)\n",
        "\n",
        "train_data = df.sample(frac=0.8, random_state=200)\n",
        "test_data = df.drop(train_data.index)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHNdHAtweQ4b"
      },
      "source": [
        "Build dictionary and transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3WkZ_kaeVYF"
      },
      "source": [
        "# Builds a dictionary of features and transforms document to feature vectors and convert tweets to a \n",
        "# matrix of token counts (CountVectorizer)\n",
        "count_vect = CountVectorizer()\n",
        "x_train_counts = count_vect.fit_transform(train_data['text'])\n",
        "\n",
        "# Transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cScZpVUMggUN"
      },
      "source": [
        "Train the model and give some new tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-2JdqdPgDjG"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
        "\n",
        "# Train classifier; train_data['category'] will be having numbers assigned for each category in train data\n",
        "clf = knn.fit(x_train_tfidf, train_data['category'])\n",
        "\n",
        "# Input data to predict their classes of the given categories\n",
        "tweets_new = [\"Seven people have been arrested following a search of two vehicles in Hayle Cornwall police have said\", \n",
        "            \"RT MoneyTelegraph Your gas and electricity bills are set to increase by 36pc over the next 10 years\",\n",
        "            \"The most common symptom of Covid19 is now a headache say experts as they warned people to get tested even if they think they are not suffering from the illness\"]\n",
        "# building up feature vector of input\n",
        "x_new_counts = count_vect.transform(tweets_new)\n",
        "# Call transform instead of fit_transform because it's already been fit\n",
        "x_new_tfidf = tfidf_transformer.transform(x_new_counts)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXqP7i0YiWo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd83974-980d-40bf-ff67-83574a2868c7"
      },
      "source": [
        "# Predicting the category of input text: Will give out number of category\n",
        "predicted = clf.predict(x_new_tfidf)\n",
        "\n",
        "for tweet, category in zip(tweets_new, predicted):\n",
        "  print('%r => %s' %(tweet, category))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Seven people have been arrested following a search of two vehicles in Hayle Cornwall police have said' => others\n",
            "'RT MoneyTelegraph Your gas and electricity bills are set to increase by 36pc over the next 10 years' => business\n",
            "'The most common symptom of Covid19 is now a headache say experts as they warned people to get tested even if they think they are not suffering from the illness' => health\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpSQ4x2Bp47O"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne_xXqtgp6EP"
      },
      "source": [
        "# use Pipeline to add vectorizer -> transformer -> classifier all in a one compound classifier\n",
        "tweet_clf = Pipeline([\n",
        "  ('vect', CountVectorizer()),\n",
        "  ('tfidf', TfidfTransformer()),\n",
        "  ('clf', knn)\n",
        "])\n",
        "\n",
        "# Fitting train data to the pipeline\n",
        "tweet_clf.fit(train_data['text'], train_data['category'])\n",
        "\n",
        "# Test data\n",
        "docs_test = test_data['text']\n",
        "\n",
        "# Predicting test data\n",
        "predicted = tweet_clf.predict(docs_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cgabdZR3Vvt"
      },
      "source": [
        "test_data_temp = preprocessor.prepare_test_dataset(test_data)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgDwHHxRSvcG",
        "outputId": "94c344d2-8fbc-4c56-8fbc-8089d3370fc1"
      },
      "source": [
        "predicted_temp = tweet_clf.predict(test_data_temp['text'])\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(test_data_temp['category'], predicted_temp, average='macro')\n",
        "accuracy = accuracy_score(test_data_temp['category'], predicted_temp)\n",
        "print(\"Precision: \", precision * 100 ,\"%\")\n",
        "print(\"Recall: \", recall * 100 ,\"%\")\n",
        "print(\"F1 Score: \", f1_score * 100 ,\"%\")\n",
        "print(\"Accuracy: \", accuracy * 100 ,\"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  85.68908733318426 %\n",
            "Recall:  77.86976691554624 %\n",
            "F1 Score:  80.17576209745275 %\n",
            "Accuracy:  82.36363636363636 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7fC0bFUBxXa"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FHx0CYByqR"
      },
      "source": [
        "pickle.dump(tweet_clf, open(\"model.pkl\", \"wb\"))"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}